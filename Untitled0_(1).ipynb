{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "sx9jMMwyUqRy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import KFold\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(256)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.fc1 = None\n",
        "        self.fc_input_features = None\n",
        "        self.fc2 = nn.Linear(1024, 512)\n",
        "        self.fc3 = nn.Linear(512, 256)\n",
        "        self.fc4 = nn.Linear(256, 128)\n",
        "        self.fc5 = nn.Linear(128, 10)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
        "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        if self.fc_input_features is None:\n",
        "            self.fc_input_features = x.size(1)\n",
        "            self.fc1 = nn.Linear(self.fc_input_features, 1024).to(x.device)\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc4(x))\n",
        "        x = self.fc5(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output\n",
        "\n",
        "\n",
        "def train(model, optimizer, train_dataloader, device):\n",
        "    model.train()\n",
        "    for data, target in train_dataloader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "def test(model, test_dataloader, device):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "        for data in test_dataloader:\n",
        "            data = data[0].to(device)\n",
        "            output = model(data)\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            predictions.extend(pred.cpu().numpy())\n",
        "    return np.array(predictions).flatten()\n",
        "\n",
        "'''def cross_validate(X, y, Xtest, ytest, n_splits=5, n_epochs=10, batch_size=32, lr=0.001, weight_decay=1e-5, patience=5):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    k_fold = KFold(n_splits=n_splits, shuffle = True, random_state=42)\n",
        "    accuracies = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(k_fold.split(X)):\n",
        "        X_train, y_train = X[train_idx], y[train_idx]\n",
        "        X_val, y_val = X[val_idx], y[val_idx]\n",
        "\n",
        "        #train model on current fold's training data\n",
        "        model = Net().to(device)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "        best_val_loss = np.inf\n",
        "        bad_epochs = 0\n",
        "\n",
        "        for epoch in range(n_epochs):\n",
        "            train(model, optimizer, DataLoader(TensorDataset(torch.Tensor(X_train), torch.LongTensor(y_train)), batch_size=batch_size, shuffle=True), device, epoch)\n",
        "            prediction_val = classify(X_val, model)\n",
        "            val_loss = accuracy_score(y_val, prediction_val)\n",
        "\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                bad_epochs = 0\n",
        "            else:\n",
        "                bad_epochs += 1\n",
        "\n",
        "            if bad_epochs >= patience:\n",
        "                break\n",
        "\n",
        "        #test model on current fold's validation data\n",
        "\n",
        "        yhat = classify(Xtest, model)\n",
        "        test_accuracy = accuracy_score(ytest, yhat)\n",
        "        accuracies.append(test_accuracy)\n",
        "        print(f\"Fold {fold + 1} Test Accuracy: {test_accuracy}\")\n",
        "\n",
        "    print(f\"Average Accuracy: {np.mean(accuracies)}\")\n",
        "    #return np.mean(accuracies)'''\n",
        "\n",
        "def cross_validate(X, y, n_splits=5, n_epochs=10, batch_size=32, lr=0.001, weight_decay=1e-6):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
        "        print(f\"Training fold {fold+1}/{n_splits}\")\n",
        "\n",
        "        X_train, X_val = X[train_idx], X[val_idx]\n",
        "        y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "        X_train = X_train.reshape(-1, 1, 28, 84)\n",
        "        X_val = X_val.reshape(-1, 1, 28, 84)\n",
        "        y_train = y_train.astype(int)\n",
        "        y_val = y_val.astype(int)\n",
        "\n",
        "        train_dataset = TensorDataset(torch.Tensor(X_train), torch.LongTensor(y_train))\n",
        "        val_dataset = TensorDataset(torch.Tensor(X_val), torch.LongTensor(y_val))\n",
        "\n",
        "        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        model = Net().to(device)\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "        # Train model\n",
        "        for epoch in range(n_epochs):\n",
        "            model.train()\n",
        "            running_loss = 0.0\n",
        "            for batch_idx, (data, target) in enumerate(train_dataloader):\n",
        "                data, target = data.to(device), target.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                output = model(data)\n",
        "                loss = F.nll_loss(output, target)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                running_loss += loss.item()\n",
        "\n",
        "            avg_train_loss = running_loss / len(train_dataloader)\n",
        "            print(f\"Epoch {epoch+1}/{n_epochs}, Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "            # Validate model\n",
        "            model.eval()\n",
        "            val_loss = 0.0\n",
        "            with torch.no_grad():\n",
        "                for data, target in val_dataloader:\n",
        "                    data, target = data.to(device), target.to(device)\n",
        "                    output = model(data)\n",
        "                    loss = F.nll_loss(output, target)\n",
        "                    val_loss += loss.item()\n",
        "\n",
        "            avg_val_loss = val_loss / len(val_dataloader)\n",
        "            print(f\"Epoch {epoch+1}/{n_epochs}, Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "def learn(X, y, n_epochs=10, batch_size=32, lr=0.001, weight_decay=1e-6):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    X = X.reshape(-1, 1, 28, 84)\n",
        "    y = y.astype(int)\n",
        "\n",
        "    dataset = TensorDataset(torch.Tensor(X), torch.LongTensor(y))\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    model = Net().to(device)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        train(model, optimizer, dataloader, device)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def classify(Xtest, model):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    Xtest = Xtest.reshape(-1, 1, 28, 84)\n",
        "    test_dataset = TensorDataset(torch.Tensor(Xtest))\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    return test(model, test_dataloader, device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepareData(file_name):\n",
        "    train_data = np.loadtxt(file_name, delimiter=',')\n",
        "    y = train_data[:, 0]\n",
        "    X = train_data[:, 1:] / 255.0\n",
        "    X = X.reshape(-1, 1, 28, 84)\n",
        "    tensor_X = torch.Tensor(X)\n",
        "    tensor_y = torch.LongTensor(y)\n",
        "    dataset = TensorDataset(tensor_X, tensor_y)\n",
        "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "    return dataset, dataloader"
      ],
      "metadata": {
        "id": "nx7E-1xGXIJV"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "\n",
        "    dataset, _ = prepareData('sample_data/A4train.csv')\n",
        "    test_dataset, test_dataloader = prepareData('sample_data/A4val.csv')\n",
        "\n",
        "\n",
        "    X, y = dataset.tensors[0].numpy(), dataset.tensors[1].numpy()\n",
        "    Xtest, ytest = test_dataset.tensors[0].numpy(), test_dataset.tensors[1].numpy()\n",
        "\n",
        "    model = cross_validate(X, y)\n",
        "    yhat = classify(Xtest, model)\n",
        "    print(\"Cross Validate Predictions:\", yhat)\n",
        "    print(\"Cross Validate Accuracy:\", accuracy_score(ytest, yhat))\n",
        "\n",
        "    model = learn(X, y)\n",
        "\n",
        "    # Classify test data\n",
        "    yhat = classify(Xtest, model)\n",
        "    print(\"CNN Predictions:\", yhat)\n",
        "    print(\"CNN Accuracy:\", accuracy_score(ytest, yhat))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XMmOd-MYU9oE"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "id": "ZMzNAO4TX7KM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47784f83-c7a8-4e35-c7d2-a59ed9ba4733"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training fold 1/5\n",
            "Epoch 1/10, Training Loss: 1.7835\n",
            "Epoch 1/10, Validation Loss: 1.2295\n",
            "Epoch 2/10, Training Loss: 0.7810\n",
            "Epoch 2/10, Validation Loss: 0.8229\n",
            "Epoch 3/10, Training Loss: 0.3992\n",
            "Epoch 3/10, Validation Loss: 0.3828\n",
            "Epoch 4/10, Training Loss: 0.2726\n",
            "Epoch 4/10, Validation Loss: 0.4363\n",
            "Epoch 5/10, Training Loss: 0.2239\n",
            "Epoch 5/10, Validation Loss: 0.4834\n",
            "Epoch 6/10, Training Loss: 0.1692\n",
            "Epoch 6/10, Validation Loss: 0.3210\n",
            "Epoch 7/10, Training Loss: 0.1526\n",
            "Epoch 7/10, Validation Loss: 0.2496\n",
            "Epoch 8/10, Training Loss: 0.1008\n",
            "Epoch 8/10, Validation Loss: 0.4160\n",
            "Epoch 9/10, Training Loss: 0.0983\n",
            "Epoch 9/10, Validation Loss: 0.5432\n",
            "Epoch 10/10, Training Loss: 0.0756\n",
            "Epoch 10/10, Validation Loss: 0.4880\n",
            "Training fold 2/5\n",
            "Epoch 1/10, Training Loss: 1.8631\n",
            "Epoch 1/10, Validation Loss: 1.1409\n",
            "Epoch 2/10, Training Loss: 0.8341\n",
            "Epoch 2/10, Validation Loss: 0.6002\n",
            "Epoch 3/10, Training Loss: 0.4452\n",
            "Epoch 3/10, Validation Loss: 0.5456\n",
            "Epoch 4/10, Training Loss: 0.3164\n",
            "Epoch 4/10, Validation Loss: 0.2970\n",
            "Epoch 5/10, Training Loss: 0.2463\n",
            "Epoch 5/10, Validation Loss: 0.4310\n",
            "Epoch 6/10, Training Loss: 0.1924\n",
            "Epoch 6/10, Validation Loss: 0.6055\n",
            "Epoch 7/10, Training Loss: 0.1559\n",
            "Epoch 7/10, Validation Loss: 0.3693\n",
            "Epoch 8/10, Training Loss: 0.1359\n",
            "Epoch 8/10, Validation Loss: 0.2454\n",
            "Epoch 9/10, Training Loss: 0.1301\n",
            "Epoch 9/10, Validation Loss: 0.8380\n",
            "Epoch 10/10, Training Loss: 0.0962\n",
            "Epoch 10/10, Validation Loss: 0.2981\n",
            "Training fold 3/5\n",
            "Epoch 1/10, Training Loss: 1.6914\n",
            "Epoch 1/10, Validation Loss: 1.0206\n",
            "Epoch 2/10, Training Loss: 0.7103\n",
            "Epoch 2/10, Validation Loss: 0.6213\n",
            "Epoch 3/10, Training Loss: 0.4105\n",
            "Epoch 3/10, Validation Loss: 0.5949\n",
            "Epoch 4/10, Training Loss: 0.2879\n",
            "Epoch 4/10, Validation Loss: 0.3861\n",
            "Epoch 5/10, Training Loss: 0.2089\n",
            "Epoch 5/10, Validation Loss: 0.3372\n",
            "Epoch 6/10, Training Loss: 0.1740\n",
            "Epoch 6/10, Validation Loss: 0.4212\n",
            "Epoch 7/10, Training Loss: 0.1412\n",
            "Epoch 7/10, Validation Loss: 0.3637\n",
            "Epoch 8/10, Training Loss: 0.1224\n",
            "Epoch 8/10, Validation Loss: 0.5605\n",
            "Epoch 9/10, Training Loss: 0.1022\n",
            "Epoch 9/10, Validation Loss: 0.3486\n",
            "Epoch 10/10, Training Loss: 0.1075\n",
            "Epoch 10/10, Validation Loss: 0.3435\n",
            "Training fold 4/5\n",
            "Epoch 1/10, Training Loss: 1.8278\n",
            "Epoch 1/10, Validation Loss: 1.1363\n",
            "Epoch 2/10, Training Loss: 0.8042\n",
            "Epoch 2/10, Validation Loss: 0.6189\n",
            "Epoch 3/10, Training Loss: 0.4046\n",
            "Epoch 3/10, Validation Loss: 0.3790\n",
            "Epoch 4/10, Training Loss: 0.2971\n",
            "Epoch 4/10, Validation Loss: 0.4173\n",
            "Epoch 5/10, Training Loss: 0.2199\n",
            "Epoch 5/10, Validation Loss: 0.2697\n",
            "Epoch 6/10, Training Loss: 0.1631\n",
            "Epoch 6/10, Validation Loss: 0.3032\n",
            "Epoch 7/10, Training Loss: 0.1477\n",
            "Epoch 7/10, Validation Loss: 0.3936\n",
            "Epoch 8/10, Training Loss: 0.1343\n",
            "Epoch 8/10, Validation Loss: 0.3159\n",
            "Epoch 9/10, Training Loss: 0.1052\n",
            "Epoch 9/10, Validation Loss: 0.3681\n",
            "Epoch 10/10, Training Loss: 0.1040\n",
            "Epoch 10/10, Validation Loss: 0.4294\n",
            "Training fold 5/5\n",
            "Epoch 1/10, Training Loss: 1.8303\n",
            "Epoch 1/10, Validation Loss: 1.1782\n",
            "Epoch 2/10, Training Loss: 0.7145\n",
            "Epoch 2/10, Validation Loss: 0.5311\n",
            "Epoch 3/10, Training Loss: 0.3942\n",
            "Epoch 3/10, Validation Loss: 0.6883\n",
            "Epoch 4/10, Training Loss: 0.2739\n",
            "Epoch 4/10, Validation Loss: 0.3562\n",
            "Epoch 5/10, Training Loss: 0.2003\n",
            "Epoch 5/10, Validation Loss: 0.4204\n",
            "Epoch 6/10, Training Loss: 0.1713\n",
            "Epoch 6/10, Validation Loss: 0.3516\n",
            "Epoch 7/10, Training Loss: 0.1452\n",
            "Epoch 7/10, Validation Loss: 0.6588\n",
            "Epoch 8/10, Training Loss: 0.1233\n",
            "Epoch 8/10, Validation Loss: 0.3411\n",
            "Epoch 9/10, Training Loss: 0.0960\n",
            "Epoch 9/10, Validation Loss: 0.3198\n",
            "Epoch 10/10, Training Loss: 0.0863\n",
            "Epoch 10/10, Validation Loss: 0.4174\n",
            "Cross Validate Predictions: [3 0 9 ... 8 4 2]\n",
            "Cross Validate Accuracy: 0.735\n"
          ]
        }
      ]
    }
  ]
}