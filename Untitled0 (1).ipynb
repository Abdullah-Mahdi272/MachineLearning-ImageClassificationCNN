{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "sx9jMMwyUqRy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import KFold\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(256)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.fc1 = None\n",
        "        self.fc_input_features = None\n",
        "        self.fc2 = nn.Linear(1024, 512)\n",
        "        self.fc3 = nn.Linear(512, 256)\n",
        "        self.fc4 = nn.Linear(256, 128)\n",
        "        self.fc5 = nn.Linear(128, 10)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
        "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        if self.fc_input_features is None:\n",
        "            self.fc_input_features = x.size(1)\n",
        "            self.fc1 = nn.Linear(self.fc_input_features, 1024).to(x.device)\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc4(x))\n",
        "        x = self.fc5(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output\n",
        "\n",
        "\n",
        "def train(model, optimizer, train_dataloader, device):\n",
        "    model.train()\n",
        "    for data, target in train_dataloader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "def test(model, test_dataloader, device):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "        for data in test_dataloader:\n",
        "            data = data[0].to(device)\n",
        "            output = model(data)\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            predictions.extend(pred.cpu().numpy())\n",
        "    return np.array(predictions).flatten()\n",
        "\n",
        "'''def cross_validate(X, y, Xtest, ytest, n_splits=5, n_epochs=10, batch_size=32, lr=0.001, weight_decay=1e-5, patience=5):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    k_fold = KFold(n_splits=n_splits, shuffle = True, random_state=42)\n",
        "    accuracies = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(k_fold.split(X)):\n",
        "        X_train, y_train = X[train_idx], y[train_idx]\n",
        "        X_val, y_val = X[val_idx], y[val_idx]\n",
        "\n",
        "        #train model on current fold's training data\n",
        "        model = Net().to(device)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "        best_val_loss = np.inf\n",
        "        bad_epochs = 0\n",
        "\n",
        "        for epoch in range(n_epochs):\n",
        "            train(model, optimizer, DataLoader(TensorDataset(torch.Tensor(X_train), torch.LongTensor(y_train)), batch_size=batch_size, shuffle=True), device, epoch)\n",
        "            prediction_val = classify(X_val, model)\n",
        "            val_loss = accuracy_score(y_val, prediction_val)\n",
        "\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                bad_epochs = 0\n",
        "            else:\n",
        "                bad_epochs += 1\n",
        "\n",
        "            if bad_epochs >= patience:\n",
        "                break\n",
        "\n",
        "        #test model on current fold's validation data\n",
        "\n",
        "        yhat = classify(Xtest, model)\n",
        "        test_accuracy = accuracy_score(ytest, yhat)\n",
        "        accuracies.append(test_accuracy)\n",
        "        print(f\"Fold {fold + 1} Test Accuracy: {test_accuracy}\")\n",
        "\n",
        "    print(f\"Average Accuracy: {np.mean(accuracies)}\")\n",
        "    #return np.mean(accuracies)'''\n",
        "\n",
        "def cross_validate(X, y, n_splits=5, n_epochs=10, batch_size=32, lr=0.001, weight_decay=1e-5, patience=3):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
        "        print(f\"Training fold {fold+1}/{n_splits}\")\n",
        "\n",
        "\n",
        "        X_train, X_val = X[train_idx], X[val_idx]\n",
        "        y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "\n",
        "        X_train = X_train.reshape(-1, 1, 28, 84)\n",
        "        X_val = X_val.reshape(-1, 1, 28, 84)\n",
        "        y_train = y_train.astype(int)\n",
        "        y_val = y_val.astype(int)\n",
        "\n",
        "        train_dataset = TensorDataset(torch.Tensor(X_train), torch.LongTensor(y_train))\n",
        "        val_dataset = TensorDataset(torch.Tensor(X_val), torch.LongTensor(y_val))\n",
        "\n",
        "        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        model = Net().to(device)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "        best_val_loss = float('inf')\n",
        "        bad_epochs = 0\n",
        "        best_model_wts = None\n",
        "\n",
        "        #train model\n",
        "        for epoch in range(n_epochs):\n",
        "            model.train()\n",
        "            running_loss = 0.0\n",
        "            for batch_idx, (data, target) in enumerate(train_dataloader):\n",
        "                data, target = data.to(device), target.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                output = model(data)\n",
        "                loss = F.nll_loss(output, target)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "\n",
        "            avg_train_loss = running_loss / len(train_dataloader)\n",
        "            print(f\"Epoch {epoch+1}/{n_epochs}, Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "            #validate model\n",
        "            model.eval()\n",
        "            val_loss = 0.0\n",
        "            with torch.no_grad():\n",
        "                for data, target in val_dataloader:\n",
        "                    data, target = data.to(device), target.to(device)\n",
        "                    output = model(data)\n",
        "                    loss = F.nll_loss(output, target)\n",
        "                    val_loss += loss.item()\n",
        "\n",
        "            avg_val_loss = val_loss / len(val_dataloader)\n",
        "            print(f\"Epoch {epoch+1}/{n_epochs}, Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "            #early stopping\n",
        "            if avg_val_loss < best_val_loss:\n",
        "                best_val_loss = avg_val_loss\n",
        "                bad_epochs = 0\n",
        "                best_model_wts = model.state_dict()\n",
        "            else:\n",
        "                bad_epochs += 1\n",
        "                print(f\"Epoch {epoch+1}: Validation loss did not improve.\")\n",
        "\n",
        "\n",
        "            if bad_epochs >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch+1} due to no improvement.\")\n",
        "                break\n",
        "\n",
        "\n",
        "        model.load_state_dict(best_model_wts)\n",
        "\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def learn(X, y, n_epochs=10, batch_size=32, lr=0.001, weight_decay=1e-5):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    X = X.reshape(-1, 1, 28, 84)\n",
        "    y = y.astype(int)\n",
        "\n",
        "    dataset = TensorDataset(torch.Tensor(X), torch.LongTensor(y))\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    model = Net().to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        train(model, optimizer, dataloader, device)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def classify(Xtest, model):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    Xtest = Xtest.reshape(-1, 1, 28, 84)\n",
        "    test_dataset = TensorDataset(torch.Tensor(Xtest))\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    return test(model, test_dataloader, device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepareData(file_name):\n",
        "    train_data = np.loadtxt(file_name, delimiter=',')\n",
        "    y = train_data[:, 0]\n",
        "    X = train_data[:, 1:] / 255.0\n",
        "    X = X.reshape(-1, 1, 28, 84)\n",
        "    tensor_X = torch.Tensor(X)\n",
        "    tensor_y = torch.LongTensor(y)\n",
        "    dataset = TensorDataset(tensor_X, tensor_y)\n",
        "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "    return dataset, dataloader"
      ],
      "metadata": {
        "id": "nx7E-1xGXIJV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "\n",
        "    dataset, _ = prepareData('sample_data/A4train.csv')\n",
        "    test_dataset, test_dataloader = prepareData('sample_data/A4val.csv')\n",
        "\n",
        "\n",
        "    X, y = dataset.tensors[0].numpy(), dataset.tensors[1].numpy()\n",
        "    Xtest, ytest = test_dataset.tensors[0].numpy(), test_dataset.tensors[1].numpy()\n",
        "\n",
        "    model = cross_validate(X, y)\n",
        "    yhat = classify(Xtest, model)\n",
        "    print(\"Cross Validate Predictions:\", yhat)\n",
        "    print(\"Cross Validate Accuracy:\", accuracy_score(ytest, yhat))\n",
        "\n",
        "    model = learn(X, y)\n",
        "\n",
        "    # Classify test data\n",
        "    yhat = classify(Xtest, model)\n",
        "    print(\"CNN Predictions:\", yhat)\n",
        "    print(\"CNN Accuracy:\", accuracy_score(ytest, yhat))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XMmOd-MYU9oE"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "id": "ZMzNAO4TX7KM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "806a8b80-40a5-43c0-d008-500d30b5adad"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training fold 1/5\n",
            "Epoch 1/10, Training Loss: 1.9359\n",
            "Epoch 1/10, Validation Loss: 1.5653\n",
            "Epoch 2/10, Training Loss: 0.9678\n",
            "Epoch 2/10, Validation Loss: 1.6677\n",
            "Epoch 2: Validation loss did not improve.\n",
            "Epoch 3/10, Training Loss: 0.5044\n",
            "Epoch 3/10, Validation Loss: 0.4103\n",
            "Epoch 4/10, Training Loss: 0.3315\n",
            "Epoch 4/10, Validation Loss: 0.3724\n",
            "Epoch 5/10, Training Loss: 0.2421\n",
            "Epoch 5/10, Validation Loss: 0.2887\n",
            "Epoch 6/10, Training Loss: 0.1903\n",
            "Epoch 6/10, Validation Loss: 0.3175\n",
            "Epoch 6: Validation loss did not improve.\n",
            "Epoch 7/10, Training Loss: 0.1713\n",
            "Epoch 7/10, Validation Loss: 0.4066\n",
            "Epoch 7: Validation loss did not improve.\n",
            "Epoch 8/10, Training Loss: 0.1269\n",
            "Epoch 8/10, Validation Loss: 0.2476\n",
            "Epoch 9/10, Training Loss: 0.1281\n",
            "Epoch 9/10, Validation Loss: 0.2800\n",
            "Epoch 9: Validation loss did not improve.\n",
            "Epoch 10/10, Training Loss: 0.1038\n",
            "Epoch 10/10, Validation Loss: 0.3410\n",
            "Epoch 10: Validation loss did not improve.\n",
            "Training fold 2/5\n",
            "Epoch 1/10, Training Loss: 1.7663\n",
            "Epoch 1/10, Validation Loss: 1.0921\n",
            "Epoch 2/10, Training Loss: 0.7230\n",
            "Epoch 2/10, Validation Loss: 0.5549\n",
            "Epoch 3/10, Training Loss: 0.3935\n",
            "Epoch 3/10, Validation Loss: 0.3490\n",
            "Epoch 4/10, Training Loss: 0.2793\n",
            "Epoch 4/10, Validation Loss: 0.2863\n",
            "Epoch 5/10, Training Loss: 0.2096\n",
            "Epoch 5/10, Validation Loss: 0.3934\n",
            "Epoch 5: Validation loss did not improve.\n",
            "Epoch 6/10, Training Loss: 0.1607\n",
            "Epoch 6/10, Validation Loss: 0.4024\n",
            "Epoch 6: Validation loss did not improve.\n",
            "Epoch 7/10, Training Loss: 0.1434\n",
            "Epoch 7/10, Validation Loss: 0.2264\n",
            "Epoch 8/10, Training Loss: 0.1226\n",
            "Epoch 8/10, Validation Loss: 0.2174\n",
            "Epoch 9/10, Training Loss: 0.1088\n",
            "Epoch 9/10, Validation Loss: 0.4468\n",
            "Epoch 9: Validation loss did not improve.\n",
            "Epoch 10/10, Training Loss: 0.0898\n",
            "Epoch 10/10, Validation Loss: 0.3312\n",
            "Epoch 10: Validation loss did not improve.\n",
            "Training fold 3/5\n",
            "Epoch 1/10, Training Loss: 1.8136\n",
            "Epoch 1/10, Validation Loss: 1.3680\n",
            "Epoch 2/10, Training Loss: 0.7610\n",
            "Epoch 2/10, Validation Loss: 0.9092\n",
            "Epoch 3/10, Training Loss: 0.4095\n",
            "Epoch 3/10, Validation Loss: 0.4437\n",
            "Epoch 4/10, Training Loss: 0.2928\n",
            "Epoch 4/10, Validation Loss: 0.4479\n",
            "Epoch 4: Validation loss did not improve.\n",
            "Epoch 5/10, Training Loss: 0.2147\n",
            "Epoch 5/10, Validation Loss: 0.3952\n",
            "Epoch 6/10, Training Loss: 0.1658\n",
            "Epoch 6/10, Validation Loss: 0.3207\n",
            "Epoch 7/10, Training Loss: 0.1350\n",
            "Epoch 7/10, Validation Loss: 0.4246\n",
            "Epoch 7: Validation loss did not improve.\n",
            "Epoch 8/10, Training Loss: 0.1256\n",
            "Epoch 8/10, Validation Loss: 0.3320\n",
            "Epoch 8: Validation loss did not improve.\n",
            "Epoch 9/10, Training Loss: 0.1028\n",
            "Epoch 9/10, Validation Loss: 0.3029\n",
            "Epoch 10/10, Training Loss: 0.0849\n",
            "Epoch 10/10, Validation Loss: 0.3721\n",
            "Epoch 10: Validation loss did not improve.\n",
            "Training fold 4/5\n",
            "Epoch 1/10, Training Loss: 1.8578\n",
            "Epoch 1/10, Validation Loss: 1.1716\n",
            "Epoch 2/10, Training Loss: 0.8919\n",
            "Epoch 2/10, Validation Loss: 1.2501\n",
            "Epoch 2: Validation loss did not improve.\n",
            "Epoch 3/10, Training Loss: 0.4324\n",
            "Epoch 3/10, Validation Loss: 0.7736\n",
            "Epoch 4/10, Training Loss: 0.2984\n",
            "Epoch 4/10, Validation Loss: 0.3193\n",
            "Epoch 5/10, Training Loss: 0.2197\n",
            "Epoch 5/10, Validation Loss: 0.6956\n",
            "Epoch 5: Validation loss did not improve.\n",
            "Epoch 6/10, Training Loss: 0.2005\n",
            "Epoch 6/10, Validation Loss: 0.4093\n",
            "Epoch 6: Validation loss did not improve.\n",
            "Epoch 7/10, Training Loss: 0.1547\n",
            "Epoch 7/10, Validation Loss: 0.3143\n",
            "Epoch 8/10, Training Loss: 0.1273\n",
            "Epoch 8/10, Validation Loss: 0.2951\n",
            "Epoch 9/10, Training Loss: 0.1108\n",
            "Epoch 9/10, Validation Loss: 0.5074\n",
            "Epoch 9: Validation loss did not improve.\n",
            "Epoch 10/10, Training Loss: 0.0987\n",
            "Epoch 10/10, Validation Loss: 0.5480\n",
            "Epoch 10: Validation loss did not improve.\n",
            "Training fold 5/5\n",
            "Epoch 1/10, Training Loss: 1.7932\n",
            "Epoch 1/10, Validation Loss: 1.4467\n",
            "Epoch 2/10, Training Loss: 0.7892\n",
            "Epoch 2/10, Validation Loss: 0.5493\n",
            "Epoch 3/10, Training Loss: 0.4268\n",
            "Epoch 3/10, Validation Loss: 0.4127\n",
            "Epoch 4/10, Training Loss: 0.2760\n",
            "Epoch 4/10, Validation Loss: 0.4419\n",
            "Epoch 4: Validation loss did not improve.\n",
            "Epoch 5/10, Training Loss: 0.2271\n",
            "Epoch 5/10, Validation Loss: 0.4501\n",
            "Epoch 5: Validation loss did not improve.\n",
            "Epoch 6/10, Training Loss: 0.1678\n",
            "Epoch 6/10, Validation Loss: 0.3274\n",
            "Epoch 7/10, Training Loss: 0.1473\n",
            "Epoch 7/10, Validation Loss: 0.7558\n",
            "Epoch 7: Validation loss did not improve.\n",
            "Epoch 8/10, Training Loss: 0.1228\n",
            "Epoch 8/10, Validation Loss: 0.3821\n",
            "Epoch 8: Validation loss did not improve.\n",
            "Epoch 9/10, Training Loss: 0.1126\n",
            "Epoch 9/10, Validation Loss: 0.3298\n",
            "Epoch 9: Validation loss did not improve.\n",
            "Early stopping at epoch 9 due to no improvement.\n",
            "Predictions: [3 0 9 ... 8 4 2]\n",
            "Accuracy: 0.7404\n",
            "Predictions: [3 0 9 ... 8 4 2]\n",
            "Accuracy: 0.7592\n"
          ]
        }
      ]
    }
  ]
}