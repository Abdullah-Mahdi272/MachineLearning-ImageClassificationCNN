{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "omrNXQo9tk98",
        "outputId": "366353af-4699-4a36-f6a3-7ef39db3bac0"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "sample_data/A4train.csv not found.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-5e9e18226fe8>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# create PyTorch DataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepareData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sample_data/A4train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepareData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sample_data/A4val.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Define CNN model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-5e9e18226fe8>\u001b[0m in \u001b[0;36mprepareData\u001b[0;34m(file_name)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Prepare dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprepareData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, quotechar, like)\u001b[0m\n\u001b[1;32m   1371\u001b[0m         \u001b[0mdelimiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1373\u001b[0;31m     arr = _read(fname, dtype=dtype, comment=comment, delimiter=delimiter,\n\u001b[0m\u001b[1;32m   1374\u001b[0m                 \u001b[0mconverters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconverters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskiplines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskiprows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1375\u001b[0m                 \u001b[0munpack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mndmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(fname, delimiter, comment, quote, imaginary_unit, usecols, skiplines, max_rows, converters, ndmin, unpack, dtype, encoding)\u001b[0m\n\u001b[1;32m    990\u001b[0m             \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 992\u001b[0;31m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    993\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m                 \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'encoding'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[1;32m    531\u001b[0m                                       encoding=encoding, newline=newline)\n\u001b[1;32m    532\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{path} not found.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: sample_data/A4train.csv not found."
          ]
        }
      ],
      "source": [
        " import time\n",
        " import numpy as np\n",
        " import torch\n",
        " import torch.nn as nn\n",
        " import torch.nn.functional as F\n",
        " import torch.optim as optim\n",
        " from torch.utils.data import TensorDataset, DataLoader\n",
        " import matplotlib.pyplot as plt\n",
        " # Prepare dataset\n",
        " def prepareData(file_name):\n",
        "     train_data = np.loadtxt(file_name, delimiter=',')\n",
        "     y = train_data[:, 0]\n",
        "     X = train_data[:, 1:] / 255.\n",
        "     # Reshape input data for single-channel images (28x84)\n",
        "     X = X.reshape(-1, 1, 28, 84)  # 1 channel, size 28x84\n",
        "     # Convert to PyTorch Tensors\n",
        "     tensor_X = torch.Tensor(X)\n",
        "     tensor_y = torch.LongTensor(y)\n",
        "     dataset = TensorDataset(tensor_X, tensor_y)  # create PyTorch TensorDataset\n",
        "     dataloader = DataLoader(dataset, batch_size=32, shuffle=True)  # create PyTorch DataLoader\n",
        "     return dataset, dataloader\n",
        " train_dataset, train_dataloader = prepareData('sample_data/A4train.csv')\n",
        " test_dataset, test_dataloader = prepareData('sample_data/A4val.csv')\n",
        " # Define CNN model\n",
        " class Net(nn.Module):\n",
        "     def __init__(self):\n",
        "         super(Net, self).__init__()\n",
        "         # Convolutional layers\n",
        "         self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)  # input: 1 channel\n",
        "         self.bn1 = nn.BatchNorm2d(32)  # batch normalization\n",
        "         self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "         self.bn2 = nn.BatchNorm2d(64)\n",
        "         self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "         self.bn3 = nn.BatchNorm2d(128)\n",
        "         self.pool = nn.MaxPool2d(kernel_size=2, stride=2)  # pooling layer\n",
        "         # Fully connected layers\n",
        "         self.fc1 = nn.Linear(128 * 3 * 10, 256)  # updated dimensions after pooling\n",
        "         self.fc2 = nn.Linear(256, 128)\n",
        "         self.fc3 = nn.Linear(128, 10)  # output layer for 10 classes\n",
        "         self.dropout = nn.Dropout(0.5)  # dropout for regularization\n",
        "     def forward(self, x):\n",
        "         x = self.conv1(x)\n",
        "         x = self.bn1(x)\n",
        "         x = F.relu(x)\n",
        "         x = self.pool(x)\n",
        "         x = self.conv2(x)\n",
        "         x = self.bn2(x)\n",
        "         x = F.relu(x)\n",
        "         x = self.pool(x)\n",
        "         x = self.conv3(x)\n",
        "         x = self.bn3(x)\n",
        "         x = F.relu(x)\n",
        "         x = self.pool(x)\n",
        "         x = x.view(x.size(0), -1)  # flatten the tensor for the fully connected layers\n",
        "         x = self.fc1(x)\n",
        "         x = F.relu(x)\n",
        "         x = self.dropout(x)\n",
        "         x = self.fc2(x)\n",
        "         x = F.relu(x)\n",
        "         x = self.dropout(x)\n",
        "         x = self.fc3(x)\n",
        "         output = F.log_softmax(x, dim=1)  # softmax for multi-class classification\n",
        "         return output\n",
        " # Device\n",
        " device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        " print(device)\n",
        " model = Net().to(device)  # use proper device\n",
        " # Optimizer: Adam\n",
        " optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        " # Training loop\n",
        " def train(model, optimizer, train_dataloader, device):\n",
        "     model.train()  # entering training mode (dropout behaves differently)\n",
        "     train_loss = 0\n",
        "     correct = 0\n",
        "     for data, target in train_dataloader:\n",
        "         data, target = data.to(device), target.to(device)  # move data to the same device\n",
        "         optimizer.zero_grad()  # clear existing gradients\n",
        "         output = model(data)  # forward pass\n",
        "         loss = F.nll_loss(output, target)  # compute the loss\n",
        "         loss.backward()  # backward pass: calculate the gradients\n",
        "         optimizer.step()  # take a gradient step\n",
        "         train_loss += loss.item() * data.size(0)\n",
        "         pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "         correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "     train_loss /= len(train_dataloader.dataset)\n",
        "     accuracy = correct / len(train_dataloader.dataset)\n",
        "     return train_loss, accuracy\n",
        " # Test loop\n",
        " def test(model, test_dataloader, device):\n",
        "     model.eval()  # entering evaluation mode (dropout behaves differently)\n",
        "     test_loss = 0\n",
        "     correct = 0\n",
        "     with torch.no_grad():  # do not compute gradient\n",
        "         for data, target in test_dataloader:\n",
        "             data, target = data.to(device), target.to(device)\n",
        "             output = model(data)\n",
        "             test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "             pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "             correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "     test_loss /= len(test_dataloader.dataset)\n",
        "     accuracy = correct / len(test_dataloader.dataset)\n",
        "     return test_loss, accuracy\n",
        " # Training with early stopping\n",
        " n_epochs = 20\n",
        " train_losses, train_accuracies = [], []\n",
        " test_losses, test_accuracies = [], []\n",
        " start_time = time.time()\n",
        " early_stop_threshold = 0.15  # Threshold for loss increase\n",
        " best_test_loss = float('inf')  # Initialize the best loss to infinity\n",
        " early_stop_triggered = False\n",
        " for epoch in range(n_epochs):\n",
        "     train_loss, train_acc = train(model, optimizer, train_dataloader, device)\n",
        "     test_loss, test_acc = test(model, test_dataloader, device)\n",
        "     train_losses.append(train_loss)\n",
        "     train_accuracies.append(train_acc)\n",
        "     test_losses.append(test_loss)\n",
        "     test_accuracies.append(test_acc)\n",
        "     print(f\"Epoch {epoch + 1}/{n_epochs}\")\n",
        "     print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.3f}\")\n",
        "     print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.3f}\")\n",
        "     # Check for significant increase in validation loss\n",
        "     if test_loss > best_test_loss + early_stop_threshold:\n",
        "         print(f\"Early stopping triggered: Validation loss increased significantly ({test_loss:.4f} > {best_test_loss + early_stop_threshold:.4f}).\")\n",
        "         early_stop_triggered = True\n",
        "         break\n",
        "     # Update the best test loss\n",
        "     best_test_loss = min(best_test_loss, test_loss)\n",
        " if not early_stop_triggered:\n",
        "     print(\"Training completed without early stopping.\")\n",
        " print(f\"Time: {time.time() - start_time:.3f} seconds\")\n",
        " # Visualization of training and validation metrics\n",
        " epochs = range(1, len(train_losses) + 1)\n",
        " plt.figure(figsize=(12, 5))\n",
        " # Plot loss\n",
        " plt.subplot(1, 2, 1)\n",
        " plt.plot(epochs, train_losses, label='Training Loss')\n",
        " plt.plot(epochs, test_losses, label='Validation Loss')\n",
        " plt.title('Loss over Epochs')\n",
        " plt.xlabel('Epochs')\n",
        " plt.ylabel('Loss')\n",
        " plt.legend()\n",
        " # Plot accuracy\n",
        " plt.subplot(1, 2, 2)\n",
        " plt.plot(epochs, train_accuracies, label='Training Accuracy')\n",
        " plt.plot(epochs, test_accuracies, label='Validation Accuracy')\n",
        " plt.title('Accuracy over Epochs')\n",
        " plt.xlabel('Epochs')\n",
        " plt.ylabel('Accuracy')\n",
        " plt.legend()\n",
        " plt.tight_layout()\n",
        " plt.show()\n",
        " for name, param in model.named_parameters():\n",
        "     if param.grad is not None:\n",
        "         print(f\"Gradient Norm of {name}: {param.grad.norm()}\")\n",
        "\n",
        "import torch\n",
        "print(torch.version.cuda)  # Should show your CUDA version"
      ]
    }
  ]
}